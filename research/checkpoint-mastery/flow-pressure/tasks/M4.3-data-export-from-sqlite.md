```yaml
task_id: M4.3
phase: M4
type: Migration
status: pending
created: 2025-11-17
```

# Task 4.3: [Migration] Data Export from SQLite

## The Constraint

**Before:** Checkpoint data locked in SQLite file → no migration path to PostgreSQL
**After:** All checkpoints exported to Python data structure → ready for PostgreSQL import

---

## The Witness

**Observable Truth:** Exported checkpoint count matches SQLite `SELECT COUNT(*)` exactly - zero data loss

**Why This Witness:**
- Complete data export with count verification can ONLY exist if export script executed successfully
- Count match proves no checkpoints were missed during export
- This witness is **measurable** (count comparison) and **verifiable** (deterministic)

---

## Acceptance Criteria

**Must Verify:**
- [ ] Export script iterates all thread_ids in SQLite
- [ ] All checkpoints for each thread_id extracted
- [ ] Exported count matches `SELECT COUNT(*) FROM checkpoints`
- [ ] Metadata preserved for each checkpoint
- [ ] Export completes in reasonable time (<1 hour for 100K checkpoints)
- [ ] Exported data structure ready for PostgreSQL import

**Cannot Exist Without:**
- Complete data extraction is **impossible** without export logic
- Count verification is **automatic** once export completes
- Export readiness is **measurable** proof of migration preparedness

---

## Code Pattern

```python
from langgraph.checkpoint.sqlite import SqliteSaver
import json
from datetime import datetime

async def export_checkpoints_from_sqlite(sqlite_path, output_file):
    """Export all checkpoints from SQLite to JSON."""

    sqlite_checkpointer = SqliteSaver.from_conn_string(sqlite_path)

    # Get all unique thread_ids
    cursor = sqlite_checkpointer.conn.cursor()
    cursor.execute("SELECT DISTINCT thread_id FROM checkpoints")
    thread_ids = [row[0] for row in cursor.fetchall()]

    print(f"Found {len(thread_ids)} unique threads")

    # Export all checkpoints
    checkpoints_to_export = []
    total_count = 0

    for thread_id in thread_ids:
        config = {"configurable": {"thread_id": thread_id}}

        for checkpoint_tuple in sqlite_checkpointer.list(config):
            checkpoints_to_export.append({
                "config": checkpoint_tuple.config,
                "checkpoint": checkpoint_tuple.checkpoint,
                "metadata": checkpoint_tuple.metadata,
                "parent_config": checkpoint_tuple.parent_config
            })
            total_count += 1

        if total_count % 1000 == 0:
            print(f"Exported {total_count} checkpoints...")

    # Verify count
    cursor.execute("SELECT COUNT(*) FROM checkpoints")
    expected_count = cursor.fetchone()[0]

    if total_count != expected_count:
        raise ValueError(
            f"Export count mismatch: exported {total_count}, "
            f"expected {expected_count}"
        )

    print(f"Export complete: {total_count} checkpoints")

    # Save to file
    with open(output_file, 'w') as f:
        json.dump({
            "export_date": datetime.now().isoformat(),
            "source": sqlite_path,
            "checkpoint_count": total_count,
            "thread_count": len(thread_ids),
            "checkpoints": checkpoints_to_export
        }, f)

    return total_count

# Usage
exported_count = await export_checkpoints_from_sqlite(
    "checkpoints.sqlite",
    "checkpoints_export.json"
)
print(f"Successfully exported {exported_count} checkpoints")
```

---

## Execution Protocol

**Prerequisites:**
- M4.2 complete (PostgreSQL schema created)
- SQLite database with checkpoint data
- Sufficient disk space for export file
- Python with langgraph installed

**Execution Steps:**
1. Create export script using pattern above
2. Run export: `python export_checkpoints.py`
3. Monitor progress (logs every 1000 checkpoints)
4. Wait for completion
5. Verify count matches SQLite
6. Check export file size (should be reasonable)
7. Backup export file (critical data)

**Verification Steps:**
1. Query SQLite count: `sqlite3 checkpoints.sqlite "SELECT COUNT(*) FROM checkpoints"`
2. Run export script
3. Check export output for count match
4. Inspect export file: `head checkpoints_export.json`
5. Verify JSON is valid: `python -m json.tool checkpoints_export.json > /dev/null`
6. Check file size is reasonable

---

## The Completion Signal

**Signal:** Export complete, count verified, data ready for import

**Evidence Required:**
- Export script output showing count match
- Export file created with all checkpoints
- Count verification: exported = SQLite count
- Export metadata (date, source, counts)

**State Transition:**
```yaml
before:
  status: pending
  witness: impossible (data locked in SQLite)
  migration: blocked

after:
  status: complete
  witness: observed (data exported with count verified)
  migration: ready for import
  evidence: [export_file, count_match, metadata]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** Export preserves all checkpoint context - thread_id, checkpoint_id, metadata, parent relationships intact.

**CONSTRAINT_INHERITANCE:** Export preserves parent-child checkpoint relationships - enables branching timeline reconstruction.

**TRACE_REQUIRED:** Export includes metadata for every checkpoint - full audit trail preserved during migration.

**RESOURCE_STEWARDSHIP:** Export to file enables verification before PostgreSQL write - no wasted database operations on bad data.

**RESIDUE_FREE:** Export file can be deleted after successful import - no permanent intermediate artifacts.

---

## Notes

**Critical Insight:** Export with verification prevents data loss during migration. Count match is non-negotiable - any mismatch requires investigation.

**Large Databases:** For >1M checkpoints, consider batched export (export per thread_id, combine later).

**Export Format:** JSON chosen for human-readability and debugging. Could use pickle for faster serialization.

**Backup Strategy:** Keep SQLite file as backup until PostgreSQL verified. Don't delete source until migration confirmed successful.
