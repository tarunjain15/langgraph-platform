```yaml
task_id: M2.2
phase: M2
type: Integration
status: complete
created: 2025-11-17
completed: 2025-11-18
```

# Task 2.2: [Integration] Blob Storage Integration

## The Constraint

**Before:** Large files (PDFs, images) stored directly in checkpoint state → 5MB checkpoint, 500ms+ serialization
**After:** Large files externalized to S3 → checkpoint contains only reference (50 bytes), <10ms serialization

---

## The Witness

**Observable Truth:** 5MB PDF upload results in checkpoint size <5KB (99.9% reduction), S3 contains the actual file

**Why This Witness:**
- A checkpoint containing only an S3 reference instead of 5MB blob data can ONLY exist if blob externalization was implemented
- The 99.9% size reduction is **impossible** to achieve any other way
- Fast checkpoint serialization (<10ms vs 500ms+) proves the blob is not being serialized
- This witness is **measurable** (checkpoint size in database) and **automatic** (every large file upload proves it)

---

## Acceptance Criteria

**Must Verify:**
- [ ] Upload 5MB PDF to graph state
- [ ] PDF uploaded to S3 successfully
- [ ] Checkpoint in SQLite/PostgreSQL contains only S3 reference (<100 bytes)
- [ ] Total checkpoint size <5KB (measured in database)
- [ ] Checkpoint serialization time <10ms (was 500ms+ with blob)
- [ ] File retrieval from S3 reference works correctly

**Cannot Exist Without:**
- Checkpoint size <5KB for 5MB file upload is **impossible** without externalization
- S3 reference in checkpoint state is **automatic** once upload pattern implemented
- 50x+ serialization speedup is **measurable** proof of blob removal

---

## Code Pattern

```python
import boto3
from typing import TypedDict

# S3 client setup
s3 = boto3.client('s3')
BUCKET_NAME = "my-app-checkpoints"

class State(TypedDict):
    uploaded_file: bytes | None  # Original file bytes (transient)
    file_reference: str | None   # S3 reference (persistent)
    file_name: str
    user_id: str

def upload_to_s3_node(state: State):
    """Upload blob to S3, return only reference."""
    file_bytes = state["uploaded_file"]

    # Generate S3 key
    key = f"uploads/{state['user_id']}/{state['file_name']}"

    # Upload to S3
    s3.put_object(
        Bucket=BUCKET_NAME,
        Key=key,
        Body=file_bytes,
        ContentType="application/pdf"
    )

    # Return ONLY reference (50 bytes vs 5MB)
    return {
        "file_reference": f"s3://{BUCKET_NAME}/{key}",
        "uploaded_file": None  # Clear blob from state
    }

def retrieve_from_s3_node(state: State):
    """Retrieve blob from S3 using reference."""
    reference = state["file_reference"]

    # Parse S3 reference
    bucket, key = reference.replace("s3://", "").split("/", 1)

    # Retrieve from S3
    response = s3.get_object(Bucket=bucket, Key=key)
    file_bytes = response['Body'].read()

    return {"uploaded_file": file_bytes}
```

---

## Execution Protocol

**Prerequisites:**
- M2.1 complete (Async checkpointer working)
- S3 bucket created with proper IAM permissions
- Boto3 installed: `pip install boto3`
- AWS credentials configured

**Execution Steps:**
1. Create S3 bucket: `aws s3 mb s3://my-app-checkpoints`
2. Configure bucket permissions (app service account needs PutObject, GetObject)
3. Add upload_to_s3_node to graph
4. Test with 5MB PDF file
5. Verify checkpoint size in database
6. Verify file exists in S3
7. Test retrieval using S3 reference

**Verification Steps:**
1. Upload test PDF: `graph.invoke({"uploaded_file": pdf_bytes, "file_name": "test.pdf"})`
2. Query checkpoint size: `SELECT LENGTH(checkpoint) FROM checkpoints WHERE thread_id = 'test-thread'`
3. Verify size <5KB (5,120 bytes)
4. Check S3: `aws s3 ls s3://my-app-checkpoints/uploads/`
5. Measure serialization time (should be <10ms)
6. Test retrieval: verify file_bytes match original

---

## The Completion Signal

**Signal:** 5MB PDF externalized, checkpoint <5KB, serialization <10ms

**Evidence Required:**
- Database query showing checkpoint size <5KB
- S3 listing showing uploaded file
- Serialization benchmark showing <10ms (vs 500ms+ baseline)
- Retrieval test proving S3 reference is valid
- Before/after comparison: 5MB → 50 bytes reference

**State Transition:**
```yaml
before:
  status: complete
  witness: impossible (5MB blobs in checkpoints)
  checkpoint_size: 5+ MB
  serialization_time: 500+ ms

after:
  status: complete
  witness: observed (S3 reference in checkpoint)
  checkpoint_size: <5 KB (99.9% reduction)
  serialization_time: <10 ms (50x speedup)
  evidence: [db_query_result, s3_listing, benchmark_output]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** S3 references are durable - file survives checkpoint pruning until explicit deletion. Reference remains valid across time.

**CONSTRAINT_INHERITANCE:** Child agents inherit S3 references - can access parent's uploaded files via reference. No blob duplication.

**TRACE_REQUIRED:** Every S3 upload traceable via checkpoint metadata - user_id + file_name + timestamp in S3 key pattern.

**RESOURCE_STEWARDSHIP:** Minimal necessary storage - blobs in S3 (cheap), only references in database (expensive). 99.9% cost reduction vs storing blobs in PostgreSQL.

**RESIDUE_FREE:** Clean separation - S3 for blobs, database for references. File deletion removes both S3 object and reference. No orphaned blobs.

---

## Notes

**Critical Insight:** This task removes the constraint of "checkpoint must contain all state". Large blobs externalized → checkpoint stays small → serialization stays fast → database stays performant.

**Size Threshold:** Only externalize files >1MB. Small files (<100KB) can stay in checkpoint state without performance impact.

**S3 Lifecycle:** Consider S3 lifecycle policies to move old uploads to Glacier or delete after N days. Coordinate with checkpoint pruning strategy.

**Security:** Use pre-signed URLs for temporary access, not permanent public URLs. S3 bucket should be private with IAM-based access control.

**Testing Gotcha:** Ensure test cleanup removes S3 objects. Don't leave 5MB test files accumulating in S3.
