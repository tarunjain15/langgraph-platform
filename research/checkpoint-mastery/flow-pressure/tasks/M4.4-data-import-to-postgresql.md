```yaml
task_id: M4.4
phase: M4
type: Migration
status: pending
created: 2025-11-17
```

# Task 4.4: [Migration] Data Import to PostgreSQL

## The Constraint

**Before:** Checkpoints exist only in export file → PostgreSQL empty
**After:** All checkpoints imported to PostgreSQL with batch insert → zero data loss verified

---

## The Witness

**Observable Truth:** PostgreSQL checkpoint count matches SQLite count exactly (M4.3 export count)

**Why This Witness:**
- Count match between PostgreSQL and SQLite can ONLY exist if import completed successfully
- Zero data loss is **measurable** via count comparison
- Batch insert performance proves efficient migration strategy

---

## Acceptance Criteria

**Must Verify:**
- [ ] Import reads export file from M4.3
- [ ] Batch insert (1000 checkpoints per batch) for performance
- [ ] All checkpoints imported without errors
- [ ] PostgreSQL count = SQLite count (zero data loss)
- [ ] Import completes in reasonable time (<2 hours for 100K checkpoints)
- [ ] Metadata preserved correctly

**Cannot Exist Without:**
- Complete import with count match is **impossible** without import logic
- Batch processing is **automatic** for performance
- Zero data loss is **measurable** via count verification

---

## Code Pattern

```python
from langgraph.checkpoint.postgres import PostgresSaver
import json

async def import_checkpoints_to_postgresql(export_file, connection_string):
    """Import all checkpoints from export file to PostgreSQL."""

    # Load export
    with open(export_file, 'r') as f:
        export_data = json.load(f)

    checkpoints = export_data['checkpoints']
    expected_count = export_data['checkpoint_count']

    print(f"Importing {expected_count} checkpoints to PostgreSQL...")

    # Create PostgreSQL checkpointer
    pg_checkpointer = PostgresSaver.from_conn_string(connection_string)

    # Batch import (1000 per batch)
    batch_size = 1000
    imported_count = 0

    for i in range(0, len(checkpoints), batch_size):
        batch = checkpoints[i:i + batch_size]

        for checkpoint_data in batch:
            await pg_checkpointer.aput(
                checkpoint_data['config'],
                checkpoint_data['checkpoint'],
                checkpoint_data['metadata'],
                new_versions={}
            )
            imported_count += 1

        print(f"Imported {imported_count}/{expected_count} checkpoints...")

    # Verify count
    conn = await asyncpg.connect(connection_string)
    pg_count = await conn.fetchval("SELECT COUNT(*) FROM checkpoints")
    await conn.close()

    if pg_count != expected_count:
        raise ValueError(
            f"Import count mismatch: PostgreSQL has {pg_count}, "
            f"expected {expected_count}"
        )

    print(f"Import complete: {pg_count} checkpoints verified")
    return pg_count

# Usage
imported_count = await import_checkpoints_to_postgresql(
    "checkpoints_export.json",
    "postgresql://user:password@host:5432/checkpoints"
)
print(f"Successfully imported {imported_count} checkpoints")
```

---

## Execution Protocol

**Prerequisites:**
- M4.3 complete (Export file created)
- M4.2 complete (PostgreSQL schema exists)
- Export file accessible
- PostgreSQL connection working

**Execution Steps:**
1. Create import script using pattern above
2. Run import: `python import_checkpoints.py`
3. Monitor progress (logs every 1000 checkpoints)
4. Wait for completion
5. Verify count matches export
6. Spot-check sample checkpoints

**Verification Steps:**
1. Run import script
2. Monitor batch progress
3. Check final count: `psql ... -c "SELECT COUNT(*) FROM checkpoints"`
4. Compare with export count
5. Verify metadata: `psql ... -c "SELECT * FROM checkpoints LIMIT 5"`

---

## The Completion Signal

**Signal:** PostgreSQL count = SQLite count, zero data loss verified

**Evidence Required:**
- Import script output showing batch progress
- Count verification query showing match
- Sample checkpoints inspected for data integrity

**State Transition:**
```yaml
before:
  status: pending
  witness: impossible (PostgreSQL empty)
  data: in export file only

after:
  status: complete
  witness: observed (all data in PostgreSQL, count verified)
  data: migrated successfully
  evidence: [import_log, count_verification, sample_inspection]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** Import preserves all checkpoint relationships - thread continuity maintained across databases.

**CONSTRAINT_INHERITANCE:** Parent-child checkpoint relationships preserved - enables branching timelines in PostgreSQL.

**TRACE_REQUIRED:** Import logs every batch - full audit trail of migration process.

**RESOURCE_STEWARDSHIP:** Batch inserts minimize database round trips - efficient use of network and CPU.

**RESIDUE_FREE:** Import is transactional - either all data migrates or none (no partial state).

---

## Notes

**Batch Size:** 1000 checkpoints per batch balances memory usage and network efficiency. Adjust based on checkpoint size.

**Transactional Import:** Consider wrapping batches in transactions - rollback on error prevents partial migration.

**Progress Tracking:** Log every batch to enable migration restart if interrupted.
