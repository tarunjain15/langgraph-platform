```yaml
task_id: M2.4
phase: M2
type: Feature
status: complete
created: 2025-11-17
completed: 2025-11-18
```

# Task 2.4: [Feature] Checkpoint Pruning Strategy

## The Constraint

**Before:** Checkpoints accumulate indefinitely → unbounded database growth → performance degradation
**After:** Automatic deletion of checkpoints older than N days → database size stabilizes

---

## The Witness

**Observable Truth:** Database size remains stable at ~500MB despite 30+ days of continuous checkpoint creation

**Why This Witness:**
- Stable database size despite continuous writes can ONLY exist if automatic pruning is working
- The absence of unbounded growth is **impossible** without pruning logic
- Orphaned writes table entries being cleaned up proves cascading deletion works
- This witness is **measurable** (database size over time) and **automatic** (daily cron job proves it)

---

## Acceptance Criteria

**Must Verify:**
- [ ] Pruning function deletes checkpoints older than 30 days
- [ ] Orphaned writes table entries removed (foreign key cascade or manual cleanup)
- [ ] Database size measured daily for 7 days - no unbounded growth
- [ ] Cron job scheduled to run pruning daily at 2am
- [ ] Pruning logs show count of deleted checkpoints
- [ ] No active threads disrupted by pruning (only expired checkpoints deleted)

**Cannot Exist Without:**
- Stable database size despite continuous writes is **impossible** without pruning
- Orphaned entry cleanup is **automatic** once cascading deletion configured
- Daily execution is **measurable** proof of cron job working

---

## Code Pattern

```python
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

async def prune_old_checkpoints(checkpointer, days=30, dry_run=False):
    """Delete checkpoints older than N days."""
    cutoff = datetime.now() - timedelta(days=days)

    # Get connection
    async with checkpointer.conn.cursor() as cursor:
        # Count checkpoints to delete
        await cursor.execute(
            "SELECT COUNT(*) FROM checkpoints WHERE ts < ?",
            (cutoff.isoformat(),)
        )
        count_to_delete = (await cursor.fetchone())[0]

        if dry_run:
            logger.info(f"DRY RUN: Would delete {count_to_delete} checkpoints older than {cutoff}")
            return count_to_delete

        # Delete old checkpoints
        await cursor.execute(
            "DELETE FROM checkpoints WHERE ts < ?",
            (cutoff.isoformat(),)
        )

        # Clean up orphaned writes (if no foreign key cascade)
        await cursor.execute(
            """
            DELETE FROM writes
            WHERE checkpoint_id NOT IN (SELECT checkpoint_id FROM checkpoints)
            """
        )

        await checkpointer.conn.commit()

        logger.info(f"Pruned {count_to_delete} checkpoints older than {cutoff}")
        return count_to_delete

# Cron job script (prune_daily.py)
async def main():
    from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

    async with AsyncSqliteSaver.from_conn_string("checkpoints.sqlite") as checkpointer:
        deleted_count = await prune_old_checkpoints(checkpointer, days=30)
        print(f"Pruned {deleted_count} checkpoints")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

# Crontab entry
# 0 2 * * * /usr/bin/python3 /path/to/prune_daily.py >> /var/log/checkpoint_pruning.log 2>&1
```

---

## Execution Protocol

**Prerequisites:**
- M2.1 complete (AsyncSqliteSaver working)
- Database with 30+ days of checkpoint history
- Cron access for scheduling daily job

**Execution Steps:**
1. Implement `prune_old_checkpoints()` function
2. Test with `dry_run=True` - verify count is correct
3. Test with `dry_run=False` - verify deletion works
4. Query database size before/after pruning
5. Create cron job script `prune_daily.py`
6. Add crontab entry: `crontab -e`
7. Verify cron job runs: check `/var/log/checkpoint_pruning.log`
8. Monitor database size daily for 7 days

**Verification Steps:**
1. Measure initial database size: `SELECT pg_size_pretty(pg_database_size('checkpoints'))`
2. Run pruning: `python prune_daily.py`
3. Measure database size after pruning (should decrease)
4. Wait 24 hours, verify cron job ran
5. Check pruning log: `tail -n 20 /var/log/checkpoint_pruning.log`
6. Continue monitoring for 7 days
7. Verify size stabilizes (no unbounded growth)

---

## The Completion Signal

**Signal:** Database size stable, pruning runs daily, no unbounded growth

**Evidence Required:**
- Database size measurements over 7 days showing stability
- Cron job logs showing daily execution
- Checkpoint count decreasing after pruning
- No active threads disrupted (only expired checkpoints deleted)
- Before/after comparison: 1GB → 500MB → stable at 500MB

**State Transition:**
```yaml
before:
  status: complete
  witness: impossible (unbounded growth)
  database_size: growing indefinitely
  pruning: manual intervention required

after:
  status: complete
  witness: observed (stable size despite continuous writes)
  database_size: stable at ~500MB
  pruning: automatic daily execution
  evidence: [size_measurements, cron_logs, deletion_counts]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** Pruning preserves recent context (30 days retention) while removing stale data. Balance between availability and storage cost.

**CONSTRAINT_INHERITANCE:** Pruning respects thread boundaries - only deletes expired checkpoints, not active threads. No orphaned child state.

**TRACE_REQUIRED:** Pruning is logged - every deletion traceable via timestamp and count. Audit trail of what was deleted when.

**RESOURCE_STEWARDSHIP:** Automatic pruning prevents storage waste - delete what's no longer needed, keep what's required. Minimal necessary retention.

**RESIDUE_FREE:** Cascading deletion ensures no orphaned writes - clean deletion leaves no debris. Database remains compact and indexed.

---

## Notes

**Critical Insight:** This task removes the constraint of "checkpoints live forever". Time-based expiration → bounded storage → predictable costs → sustainable operation.

**Retention Policy:** 30 days is default - adjust based on business requirements. Longer retention = more storage cost. Shorter retention = less recovery capability.

**Thread Lifecycle:** Consider thread-level retention instead of global retention. Active threads = keep forever, inactive threads = prune after 30 days.

**Backup Consideration:** Ensure backups capture pre-pruned data. Don't prune before backup window expires.

**Testing Gotcha:** Test pruning on non-production data first. Accidental deletion of active threads is irreversible.

**Performance Note:** Pruning on large databases can be slow - consider batching deletions (1000 checkpoints per batch) to avoid long-running transactions.

**Monitoring Alert:** Set alert if database size grows >1GB despite pruning - indicates pruning failure or retention policy too generous.
