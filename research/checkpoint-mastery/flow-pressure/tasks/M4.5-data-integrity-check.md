```yaml
task_id: M4.5
phase: M4
type: Migration
status: pending
created: 2025-11-17
```

# Task 4.5: [Migration] Data Integrity Check

## The Constraint

**Before:** Import complete but data integrity unknown → risk of corrupted migration
**After:** Random sample verified (SQLite state == PostgreSQL state) → 100% integrity confirmed

---

## The Witness

**Observable Truth:** 10 randomly sampled threads show identical checkpoint data in SQLite and PostgreSQL

**Why This Witness:**
- Exact state match across databases can ONLY exist if migration preserved data integrity
- Random sampling proves systematic correctness (not cherry-picked success)
- 100% match rate is **measurable** and **verifiable**

---

## Acceptance Criteria

**Must Verify:**
- [ ] Random sample of 10 threads selected
- [ ] For each thread, compare all checkpoints
- [ ] Checkpoint IDs match exactly
- [ ] Checkpoint content matches exactly
- [ ] Metadata matches exactly
- [ ] 100% of sampled checkpoints match (no errors tolerated)

**Cannot Exist Without:**
- Perfect data match is **impossible** with corrupted migration
- Random sampling is **automatic** verification strategy
- 100% success rate is **measurable** proof of integrity

---

## Code Pattern

```python
import random
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.checkpoint.postgres import PostgresSaver

async def verify_migration_integrity(sqlite_path, pg_conn_string, sample_size=10):
    """Verify data integrity by comparing random sample."""

    sqlite_checkpointer = SqliteSaver.from_conn_string(sqlite_path)
    pg_checkpointer = PostgresSaver.from_conn_string(pg_conn_string)

    # Get all thread_ids from SQLite
    cursor = sqlite_checkpointer.conn.cursor()
    cursor.execute("SELECT DISTINCT thread_id FROM checkpoints")
    all_threads = [row[0] for row in cursor.fetchall()]

    # Random sample
    sample_threads = random.sample(all_threads, min(sample_size, len(all_threads)))
    print(f"Verifying {len(sample_threads)} randomly sampled threads...")

    mismatches = []

    for thread_id in sample_threads:
        config = {"configurable": {"thread_id": thread_id}}

        # Get checkpoints from both databases
        sqlite_checkpoints = list(sqlite_checkpointer.list(config))
        pg_checkpoints = list(pg_checkpointer.list(config))

        # Compare counts
        if len(sqlite_checkpoints) != len(pg_checkpoints):
            mismatches.append({
                "thread_id": thread_id,
                "error": "checkpoint count mismatch",
                "sqlite_count": len(sqlite_checkpoints),
                "pg_count": len(pg_checkpoints)
            })
            continue

        # Compare each checkpoint
        for i, (sqlite_cp, pg_cp) in enumerate(zip(sqlite_checkpoints, pg_checkpoints)):
            # Compare checkpoint IDs
            if sqlite_cp.config != pg_cp.config:
                mismatches.append({
                    "thread_id": thread_id,
                    "checkpoint_index": i,
                    "error": "config mismatch"
                })

            # Compare checkpoint content
            if sqlite_cp.checkpoint != pg_cp.checkpoint:
                mismatches.append({
                    "thread_id": thread_id,
                    "checkpoint_index": i,
                    "error": "checkpoint content mismatch"
                })

            # Compare metadata
            if sqlite_cp.metadata != pg_cp.metadata:
                mismatches.append({
                    "thread_id": thread_id,
                    "checkpoint_index": i,
                    "error": "metadata mismatch"
                })

        print(f"✓ Thread {thread_id}: {len(sqlite_checkpoints)} checkpoints verified")

    # Report results
    if mismatches:
        print(f"\n❌ INTEGRITY CHECK FAILED: {len(mismatches)} mismatches found")
        for mismatch in mismatches:
            print(f"  {mismatch}")
        raise ValueError("Migration integrity check failed")

    print(f"\n✓ INTEGRITY CHECK PASSED: All {len(sample_threads)} threads verified")
    return True

# Usage
await verify_migration_integrity(
    "checkpoints.sqlite",
    "postgresql://user:password@host:5432/checkpoints",
    sample_size=10
)
```

---

## Execution Protocol

**Prerequisites:**
- M4.4 complete (Import finished)
- Both SQLite and PostgreSQL accessible
- Random sampling script ready

**Execution Steps:**
1. Create verification script
2. Run with sample_size=10
3. Wait for verification
4. Check for any mismatches
5. If mismatches found, investigate and fix
6. Re-run until 100% match

**Verification Steps:**
1. Run verification script
2. Monitor output for each thread
3. Check final result (pass/fail)
4. If failed, inspect mismatch details
5. Verify 100% success rate

---

## The Completion Signal

**Signal:** 100% of sampled checkpoints match exactly

**Evidence Required:**
- Verification script output showing all threads passed
- No mismatches reported
- Sample size documented (10 threads minimum)

**State Transition:**
```yaml
before:
  status: pending
  witness: impossible (integrity unknown)
  confidence: uncertain

after:
  status: complete
  witness: observed (100% match on random sample)
  confidence: high (data integrity verified)
  evidence: [verification_output, sample_threads, match_rate]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** Integrity check verifies context preservation across migration - all relationships intact.

**CONSTRAINT_INHERITANCE:** Verification confirms parent-child checkpoint relationships preserved.

**TRACE_REQUIRED:** Verification process logged - full audit trail of integrity check.

**RESOURCE_STEWARDSHIP:** Random sampling efficient - verify integrity without checking every checkpoint.

**RESIDUE_FREE:** Verification is read-only - no side effects on either database.

---

## Notes

**Sample Size:** 10 threads gives 95% confidence. Increase to 30 for 99% confidence on large datasets.

**Deterministic Verification:** For critical migrations, verify ALL threads instead of sample. Takes longer but 100% certainty.
