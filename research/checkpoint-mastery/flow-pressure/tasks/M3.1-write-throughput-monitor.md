```yaml
task_id: M3.1
phase: M3
type: Integration
status: pending
created: 2025-11-17
```

# Task 3.1: [Integration] Write Throughput Monitor

## The Constraint

**Before:** No visibility into write capacity → hit ceiling blindly → silent degradation
**After:** Dashboard shows current write throughput + error rate → know when to scale

---

## The Witness

**Observable Truth:** Dashboard displays real-time metrics: writes/second, error rate (%), P99 latency, with alerts firing when thresholds exceeded

**Why This Witness:**
- Real-time metrics dashboard showing checkpoint writes can ONLY exist if instrumentation was added
- Alert firing when error rate >1% proves threshold monitoring works
- The ability to observe system saturation before failure is **impossible** without metrics collection
- This witness is **measurable** (dashboard shows numbers) and **automatic** (metrics update continuously)

---

## Acceptance Criteria

**Must Verify:**
- [ ] Prometheus/CloudWatch metrics collection working
- [ ] Dashboard shows writes/second (current throughput)
- [ ] Dashboard shows error rate percentage (failures/total)
- [ ] Dashboard shows P99 latency (99th percentile write time)
- [ ] Alert triggers when error rate >1%
- [ ] Alert triggers when throughput >80 writes/sec (approaching SQLite ceiling)
- [ ] Metrics retained for 7 days minimum (trend analysis)

**Cannot Exist Without:**
- Real-time throughput visibility is **impossible** without metrics instrumentation
- Alert triggering at thresholds is **automatic** once monitoring configured
- Trend analysis is **measurable** proof of retention working

---

## Code Pattern

```python
from prometheus_client import Counter, Histogram, generate_latest
import time
import logging

logger = logging.getLogger(__name__)

# Prometheus metrics
checkpoint_writes_total = Counter(
    'checkpoint_writes_total',
    'Total number of checkpoint writes',
    ['status']  # success or error
)

checkpoint_write_duration_seconds = Histogram(
    'checkpoint_write_duration_seconds',
    'Checkpoint write duration in seconds',
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
)

class MonitoredCheckpointer:
    """Wrapper that instruments checkpointer with metrics."""

    def __init__(self, checkpointer):
        self.checkpointer = checkpointer

    async def aput(self, config, checkpoint, metadata, new_versions):
        """Store checkpoint with metrics collection."""
        start_time = time.time()

        try:
            # Attempt checkpoint write
            result = await self.checkpointer.aput(config, checkpoint, metadata, new_versions)

            # Record success
            checkpoint_writes_total.labels(status='success').inc()
            duration = time.time() - start_time
            checkpoint_write_duration_seconds.observe(duration)

            return result

        except Exception as e:
            # Record error
            checkpoint_writes_total.labels(status='error').inc()
            logger.error(f"Checkpoint write failed: {e}")
            raise

# Expose metrics endpoint
from flask import Flask, Response

app = Flask(__name__)

@app.route('/metrics')
def metrics():
    return Response(generate_latest(), mimetype='text/plain')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=9090)

# Prometheus queries for dashboard
"""
# Write throughput (writes per second)
rate(checkpoint_writes_total[1m])

# Error rate (percentage)
rate(checkpoint_writes_total{status="error"}[1m]) /
rate(checkpoint_writes_total[1m]) * 100

# P99 latency
histogram_quantile(0.99, rate(checkpoint_write_duration_seconds_bucket[5m]))

# Total writes
sum(checkpoint_writes_total)
"""

# AlertManager rules
"""
groups:
  - name: checkpoint_alerts
    rules:
      - alert: HighErrorRate
        expr: |
          rate(checkpoint_writes_total{status="error"}[5m]) /
          rate(checkpoint_writes_total[5m]) > 0.01
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Checkpoint error rate >1%"
          description: "Error rate: {{ $value }}%"

      - alert: ApproachingCapacity
        expr: rate(checkpoint_writes_total[1m]) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Write throughput >80/sec (SQLite ceiling)"
          description: "Current: {{ $value }} writes/sec"
"""
```

---

## Execution Protocol

**Prerequisites:**
- M2 complete (Async checkpointer working)
- Prometheus installed (or CloudWatch access)
- Grafana installed for dashboard visualization
- AlertManager configured for notifications

**Execution Steps:**
1. Install prometheus_client: `pip install prometheus-client`
2. Implement `MonitoredCheckpointer` wrapper
3. Expose metrics endpoint at `/metrics`
4. Configure Prometheus to scrape `/metrics` endpoint
5. Create Grafana dashboard with 3 panels:
   - Write throughput (writes/sec)
   - Error rate (%)
   - P99 latency (ms)
6. Configure AlertManager rules for thresholds
7. Test alert firing by simulating high error rate
8. Verify 7-day retention in Prometheus

**Verification Steps:**
1. Start metrics endpoint: `python metrics_server.py`
2. Verify metrics exposed: `curl localhost:9090/metrics`
3. Configure Prometheus scrape: add to `prometheus.yml`
4. Verify Prometheus collecting data: check targets page
5. Create Grafana dashboard: import dashboard JSON
6. Verify metrics displaying correctly
7. Test alert: simulate >1% error rate
8. Verify alert fires and notification sent

---

## The Completion Signal

**Signal:** Dashboard showing metrics, alerts configured and tested

**Evidence Required:**
- Screenshot of Grafana dashboard showing all 3 metrics
- Prometheus query results showing data collection
- AlertManager configuration showing threshold rules
- Test alert firing proof (notification screenshot)
- 7-day data retention verified

**State Transition:**
```yaml
before:
  status: pending
  witness: impossible (no visibility)
  monitoring: blind operation
  capacity_awareness: none

after:
  status: complete
  witness: observed (dashboard showing real-time metrics)
  monitoring: instrumented with alerts
  capacity_awareness: full visibility
  evidence: [dashboard_screenshot, prometheus_data, alert_test]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** Metrics retained for 7 days enable temporal analysis - understand system behavior over time. Historical context preserved.

**CONSTRAINT_INHERITANCE:** Child agents inherit instrumentation - all checkpoint writes measured. No blind spots in monitoring.

**TRACE_REQUIRED:** Every checkpoint write traceable via metrics - timestamp + status + duration. Full audit trail of system performance.

**RESOURCE_STEWARDSHIP:** Minimal instrumentation overhead (<1ms per write). Metrics collection uses minimal resources. Efficient observability.

**RESIDUE_FREE:** Metrics auto-expire after 7 days - no unbounded growth. Clean retention policy prevents metric bloat.

---

## Notes

**Critical Insight:** This task removes the constraint of "operating blind". Metrics visibility → know capacity ceiling → scale before failure → proactive operation.

**Why These Metrics:**
- **Writes/sec:** Know current throughput vs capacity ceiling
- **Error rate:** Know when system is saturated
- **P99 latency:** Know user experience (99% of writes complete in X ms)

**SQLite Ceiling:** ~100 writes/sec sustained. Alert at 80 writes/sec to scale proactively.

**CloudWatch Alternative:** If using AWS, CloudWatch custom metrics work instead of Prometheus:
```python
cloudwatch.put_metric_data(
    Namespace='Checkpoints',
    MetricData=[{
        'MetricName': 'WriteCount',
        'Value': 1,
        'Unit': 'Count'
    }]
)
```

**Testing Gotcha:** Don't test alerts with real traffic - use synthetic load generator to simulate high error rate.

**Dashboard Design:** Keep dashboard simple - 3 metrics only. Avoid metric overload that obscures signal.

**Alert Fatigue:** Set alert thresholds conservatively - only alert on actionable conditions. False positives reduce trust.
