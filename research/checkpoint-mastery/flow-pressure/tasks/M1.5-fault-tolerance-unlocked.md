```yaml
task_id: M1.5
phase: M1
type: Feature
status: complete
created: 2025-11-17
completed: 2025-11-17
```

# Task 1.5: [Feature] Fault Tolerance Unlocked

## The Constraint

**Before:** Process crash = complete data loss, must restart from beginning
**After:** Process crash → resume with same `thread_id` → execution continues from last checkpoint, skipping completed nodes

---

## The Witness

**Observable Truth:** 5-node graph execution, simulated crash after node 2, resume with same thread_id → nodes 1-2 skipped (already completed), execution continues from node 3

**Why This Witness:**
- Crash recovery with work preservation is **impossible** without persistent checkpoints
- The ability to skip completed nodes and resume from failure point can ONLY happen if checkpoints track execution progress
- This is not a feature we built - it **emerged** from M1.1's infrastructure
- The witness is the **skipped execution** - if nodes 1-2 don't re-run, fault tolerance emerged

---

## Acceptance Criteria

**Must Verify:**
- [ ] Create 5-node linear graph (node1 → node2 → node3 → node4 → node5)
- [ ] Each node logs execution with timestamp
- [ ] Run graph through node 2, verify checkpoint created
- [ ] Simulate crash (terminate process, or raise exception)
- [ ] Resume graph with same `thread_id` (new process or exception recovery)
- [ ] Verify nodes 1-2 do NOT re-execute (check logs - no new timestamps)
- [ ] Verify nodes 3-5 DO execute (check logs - new timestamps)
- [ ] Verify final state matches expected result (as if no crash occurred)

**Cannot Exist Without:**
- Execution progress tracking is **impossible** without persistent checkpoints
- Work preservation across crashes is **automatic** once checkpoints exist
- Fault Tolerance is **zero additional code** - emerged from M1.1

---

## Code Pattern

```python
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, START, END
from datetime import datetime

# Setup (assumes M1.1 complete)
checkpointer = SqliteSaver.from_conn_string("checkpoints.sqlite")

# Graph with observable execution steps
class State(TypedDict):
    value: int
    log: list[str]

def node1(state: State):
    timestamp = datetime.now().isoformat()
    print(f"[{timestamp}] NODE 1 executing")
    return {
        "value": state["value"] + 1,
        "log": state["log"] + [f"node1 at {timestamp}"]
    }

def node2(state: State):
    timestamp = datetime.now().isoformat()
    print(f"[{timestamp}] NODE 2 executing")
    return {
        "value": state["value"] + 10,
        "log": state["log"] + [f"node2 at {timestamp}"]
    }

def node3(state: State):
    timestamp = datetime.now().isoformat()
    print(f"[{timestamp}] NODE 3 executing")
    return {
        "value": state["value"] + 100,
        "log": state["log"] + [f"node3 at {timestamp}"]
    }

def node4(state: State):
    timestamp = datetime.now().isoformat()
    print(f"[{timestamp}] NODE 4 executing")
    return {
        "value": state["value"] + 1000,
        "log": state["log"] + [f"node4 at {timestamp}"]
    }

def node5(state: State):
    timestamp = datetime.now().isoformat()
    print(f"[{timestamp}] NODE 5 executing")
    return {
        "value": state["value"] + 10000,
        "log": state["log"] + [f"node5 at {timestamp}"]
    }

# Build linear graph
builder = StateGraph(State)
builder.add_node("node1", node1)
builder.add_node("node2", node2)
builder.add_node("node3", node3)
builder.add_node("node4", node4)
builder.add_node("node5", node5)

builder.add_edge(START, "node1")
builder.add_edge("node1", "node2")
builder.add_edge("node2", "node3")
builder.add_edge("node3", "node4")
builder.add_edge("node4", "node5")
builder.add_edge("node5", END)

graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "fault-tolerance-test"}}

# === PART 1: Run until crash ===
print("=== Starting execution ===")
try:
    # Run through node 2, then simulate crash
    result = graph.invoke({"value": 0, "log": []}, config)

    # Check state after node2
    state_after_node2 = graph.get_state(config)
    print(f"\nState after node2:")
    print(f"  value: {state_after_node2.values['value']}")
    print(f"  log entries: {len(state_after_node2.values['log'])}")

    # Simulate crash (in real scenario, process would terminate here)
    raise Exception("SIMULATED CRASH")

except Exception as e:
    print(f"\n!!! CRASH: {e} !!!")
    print("Process would terminate here in real scenario")

# === PART 2: Resume from checkpoint ===
print("\n=== Resuming from checkpoint ===")

# Get state before resume
state_before_resume = graph.get_state(config)
print(f"Resuming from: {state_before_resume.next}")
print(f"Value before resume: {state_before_resume.values['value']}")
print(f"Log entries before resume: {len(state_before_resume.values['log'])}")

# Resume execution
# Key insight: nodes 1-2 will NOT re-execute
final_result = graph.invoke(None, config)  # None = resume from checkpoint

print(f"\n=== Final result ===")
print(f"Final value: {final_result['value']}")
# Expected: 11111 (1 + 10 + 100 + 1000 + 10000)

print(f"\nExecution log:")
for entry in final_result['log']:
    print(f"  {entry}")

# Verify nodes 1-2 only executed once
print(f"\nTotal log entries: {len(final_result['log'])}")
# Expected: 5 entries (one per node)

# Check if node1 and node2 have only one timestamp each
node1_executions = [e for e in final_result['log'] if 'node1' in e]
node2_executions = [e for e in final_result['log'] if 'node2' in e]
print(f"Node1 executions: {len(node1_executions)} (expected: 1)")
print(f"Node2 executions: {len(node2_executions)} (expected: 1)")
```

---

## Execution Protocol

**Prerequisites:**
- M1.1 complete (`checkpoints.sqlite` exists with schema)
- LangGraph graph with multiple nodes in sequence
- Each node produces observable side effects (logs, state changes)

**Execution Steps:**
1. Create multi-node graph with logging in each node
2. Compile graph with checkpointer
3. Run graph through partial execution (e.g., first 2 nodes)
4. Verify checkpoint created (query checkpoint history)
5. Simulate crash (terminate process or raise exception)
6. Resume graph with same `thread_id` using `graph.invoke(None, config)`
7. Verify completed nodes do NOT re-execute (check logs)
8. Verify remaining nodes DO execute
9. Verify final state is correct (as if no crash occurred)

**Verification Steps:**
1. Count log entries after crash - should be 2 (node1 + node2)
2. Resume execution
3. Count log entries after resume - should be 5 total (node1-5), NOT 8 (which would indicate re-execution)
4. Check timestamps - node1 and node2 should have single timestamp (no re-execution)
5. Check final value - should be 11111 (sum of all increments)
6. Query checkpoint history - should show checkpoint after node2, then final checkpoint

---

## The Completion Signal

**Signal:** Crash recovery successful (nodes 1-2 skipped, execution resumed from node 3)

**Evidence Required:**
- Terminal output showing nodes 1-2 executing BEFORE crash
- Terminal output showing simulated crash
- Terminal output showing resume starting from node 3 (NOT node 1)
- Execution log showing exactly 5 entries (one per node, no duplicates)
- Final value is 11111 (proves all nodes executed exactly once)
- Checkpoint history showing: checkpoint after node2, checkpoint after node5

**State Transition:**
```yaml
before:
  status: complete
  witness: impossible (crash = data loss, restart from beginning)
  fault_tolerance: blocked
  crash_recovery: none

after:
  status: complete
  witness: observed (nodes 1-2 skipped on resume, no re-execution)
  fault_tolerance: emerged (zero code written for this feature)
  crash_recovery: automatic (work preserved, execution resumed)
  evidence: [execution log, checkpoint history, no duplicate execution]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** Fault tolerance proves checkpoint reliability - crash does not lose context, resume is seamless

**CONSTRAINT_INHERITANCE:** Crashed child agents can resume with inherited constraints intact - constraint propagation survives crashes

**TRACE_REQUIRED:** Every crash and resume is traced - checkpoint history shows exact failure point and recovery point

**RESOURCE_STEWARDSHIP:** No wasted computation - completed work is never re-executed, only missing work is performed

**RESIDUE_FREE:** Crash recovery is clean - no orphaned state, no partial writes, checkpoint integrity maintained

---

## Notes

**Critical Insight:** We did NOT build Fault Tolerance. We verified it **emerged** from M1.1's checkpoint mechanism. The graph runtime automatically skips completed nodes based on checkpoint state.

**False Emergence Warning:** Do NOT add code for "crash recovery" or "execution state tracking" or "work preservation". If you find yourself writing fault tolerance logic, you've violated the emergence principle. Fault Tolerance should work with ZERO additional code beyond M1.1.

**How It Works Internally:**
1. Each node execution creates a checkpoint
2. Checkpoint contains `next` field (list of pending nodes)
3. On resume, graph checks `next` - if empty, graph is complete
4. If `next` has nodes, execute those nodes, skip completed ones
5. Completed nodes are identified by absence from `next` list

**Real-World Crash Scenarios:**
- Server restart (process termination)
- OOM kill (out of memory)
- Network timeout (long-running LLM call fails)
- Exception in node (Python error)
- Kubernetes pod eviction
- Manual termination (Ctrl+C)

**Testing True Process Termination:**
For full witness verification, split execution into two separate Python scripts:

**Script 1 (run_until_crash.py):**
```python
# Run graph through node 2, then exit
result = graph.invoke({"value": 0, "log": []}, config)
print("Completed node 2, process exiting...")
```

**Script 2 (resume_after_crash.py):**
```python
# Resume from checkpoint
final_result = graph.invoke(None, config)
print(f"Final result: {final_result['value']}")
```

Run script 1, verify it completes, then run script 2. If node 1-2 don't re-execute, true fault tolerance is proven.

**Idempotency Note:** Fault tolerance assumes nodes are **not idempotent** - they produce side effects (API calls, DB writes, etc). If nodes were idempotent, re-execution would be safe but wasteful. Checkpointing prevents waste by skipping completed work.

**Long-Running Nodes:** If a node takes 10 minutes and crashes at minute 9, that node WILL re-execute on resume (checkpoint is created AFTER node completion, not during). For long-running nodes, break them into smaller sub-nodes to create more frequent checkpoints.
