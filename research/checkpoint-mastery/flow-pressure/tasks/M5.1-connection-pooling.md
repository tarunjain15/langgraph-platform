```yaml
task_id: M5.1
phase: M5
type: Optimization
status: pending
created: 2025-11-17
```

# Task 5.1: [Optimization] Connection Pooling

## The Constraint

**Before:** New PostgreSQL connection per checkpoint write → 50ms connection overhead
**After:** Connection pool reuses connections → 5ms connection acquisition

---

## The Witness

**Observable Truth:** Connection acquisition time: 50ms → 5ms (10x improvement measured)

**Why This Witness:**
- Sub-10ms connection acquisition can ONLY exist if connection pooling implemented
- 10x speedup is **measurable** via latency metrics
- Throughput increase proves pool working

---

## Acceptance Criteria

**Must Verify:**
- [ ] Connection pool configured (min=10, max=100 connections)
- [ ] Connection acquisition <10ms (was 50ms+)
- [ ] Pool metrics showing reuse rate >90%
- [ ] No connection exhaustion under load
- [ ] Graceful degradation when pool saturated

**Cannot Exist Without:**
- Sub-10ms acquisition is **impossible** without pooling
- Connection reuse is **automatic** once pool configured
- Throughput improvement is **measurable**

---

## Code Pattern

```python
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
import asyncpg

# Connection pool configuration
pool = await asyncpg.create_pool(
    "postgresql://user:password@host:5432/checkpoints",
    min_size=10,      # Minimum connections kept alive
    max_size=100,     # Maximum concurrent connections
    command_timeout=60,
    max_queries=50000,  # Recycle connections after 50K queries
    max_inactive_connection_lifetime=300  # 5 min idle timeout
)

# Use with checkpointer
checkpointer = AsyncPostgresSaver(pool=pool)

# Metrics
from prometheus_client import Histogram

connection_acquisition_seconds = Histogram(
    'checkpoint_connection_acquisition_seconds',
    'Time to acquire database connection',
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1]
)

async def monitored_checkpoint_write(checkpointer, config, checkpoint, metadata):
    start = time.time()
    # Connection acquired from pool here
    result = await checkpointer.aput(config, checkpoint, metadata, {})
    duration = time.time() - start
    connection_acquisition_seconds.observe(duration)
    return result
```

---

## Execution Protocol

**Prerequisites:**
- M4.6 complete (PostgreSQL in production)
- asyncpg installed: `pip install asyncpg`
- Baseline metrics collected (50ms connection time)

**Execution Steps:**
1. Configure connection pool (min=10, max=100)
2. Update checkpointer to use pool
3. Deploy to production
4. Measure connection acquisition time
5. Verify <10ms (vs 50ms baseline)
6. Monitor pool saturation metrics

**Verification Steps:**
1. Benchmark before: connection time ~50ms
2. Enable connection pool
3. Benchmark after: connection time <10ms
4. Check pool metrics: `SELECT * FROM pg_stat_database`
5. Verify reuse rate >90%

---

## The Completion Signal

**Signal:** Connection acquisition <10ms, 10x improvement verified

**Evidence Required:**
- Before/after latency comparison (50ms → 5ms)
- Pool metrics showing reuse rate
- Throughput improvement measured

**State Transition:**
```yaml
before:
  status: pending
  connection_time: 50ms
  pooling: none

after:
  status: complete
  connection_time: 5ms (10x improvement)
  pooling: enabled (10-100 connections)
  evidence: [latency_comparison, pool_metrics]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** Connection pooling preserves transaction isolation - each checkpoint write atomic.

**CONSTRAINT_INHERITANCE:** Pool shared across all graph invocations - efficient resource sharing.

**TRACE_REQUIRED:** Pool metrics logged - connection count, wait time, saturation events.

**RESOURCE_STEWARDSHIP:** Pool prevents connection waste - reuse instead of recreate. Max limit prevents exhaustion.

**RESIDUE_FREE:** Connections recycled automatically - no leaked connections.

---

## Notes

**Pool Sizing:** min=10 keeps connections warm. max=100 prevents PostgreSQL saturation (default max_connections=100).

**Tuning:** Adjust based on load. High throughput = higher min. Low throughput = lower min to save resources.
