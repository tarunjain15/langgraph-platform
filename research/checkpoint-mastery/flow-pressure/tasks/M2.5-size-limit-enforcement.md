```yaml
task_id: M2.5
phase: M2
type: Integration
status: complete
created: 2025-11-17
completed: 2025-11-18
```

# Task 2.5: [Integration] Size Limit Enforcement

## The Constraint

**Before:** No checkpoint size validation → 15MB states crash serialization → silent failures
**After:** Checkpoint serialization rejects >10MB states → fast fail with helpful error

---

## The Witness

**Observable Truth:** Attempt to save 15MB checkpoint results in clear error message before serialization, no checkpoint >10MB exists in database

**Why This Witness:**
- Rejection of 15MB checkpoint with helpful error can ONLY exist if size validation was implemented
- The absence of any checkpoint >10MB in database proves enforcement is working
- Fast fail before serialization (not during/after) proves validation happens early
- This witness is **measurable** (max checkpoint size in DB) and **automatic** (every large checkpoint attempt proves it)

---

## Acceptance Criteria

**Must Verify:**
- [ ] Attempt to save 15MB checkpoint
- [ ] Rejection with clear error message explaining size limit
- [ ] Error message includes current size (15MB) and limit (10MB)
- [ ] Error message suggests blob externalization to S3
- [ ] No checkpoint >10MB exists in database (verified via query)
- [ ] Size check happens before serialization (fast fail, not slow fail)
- [ ] Existing <10MB checkpoints continue working normally

**Cannot Exist Without:**
- 15MB checkpoint rejection is **impossible** without size validation
- Zero checkpoints >10MB in database is **automatic** once enforcement exists
- Fast fail is **measurable** proof of early validation

---

## Code Pattern

```python
from langgraph.checkpoint.base import Checkpoint
import logging

logger = logging.getLogger(__name__)

class SizeLimitedCheckpointer:
    """Wrapper that enforces checkpoint size limits."""

    def __init__(self, checkpointer, max_size_mb=10):
        self.checkpointer = checkpointer
        self.max_size_mb = max_size_mb
        self.max_size_bytes = max_size_mb * 1024 * 1024

    def validate_checkpoint_size(self, checkpoint: Checkpoint):
        """Validate checkpoint size before serialization."""
        # Serialize to measure size
        serialized = self.checkpointer.serde.dumps(checkpoint)
        size_bytes = len(serialized)
        size_mb = size_bytes / (1024 ** 2)

        if size_bytes > self.max_size_bytes:
            raise ValueError(
                f"Checkpoint size {size_mb:.2f}MB exceeds limit {self.max_size_mb}MB. "
                f"Current size: {size_bytes:,} bytes, Limit: {self.max_size_bytes:,} bytes. "
                f"Consider externalizing large blobs to S3 (see M2.2-blob-storage-integration.md). "
                f"Large state detected in checkpoint - use S3 references instead of raw bytes."
            )

        logger.debug(f"Checkpoint size: {size_mb:.2f}MB (within {self.max_size_mb}MB limit)")
        return size_mb

    async def aput(self, config, checkpoint, metadata, new_versions):
        """Store checkpoint with size validation."""
        # Validate size BEFORE storage
        size_mb = self.validate_checkpoint_size(checkpoint)

        # Proceed with storage
        return await self.checkpointer.aput(config, checkpoint, metadata, new_versions)

# Usage
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

async with AsyncSqliteSaver.from_conn_string("checkpoints.sqlite") as base_checkpointer:
    checkpointer = SizeLimitedCheckpointer(base_checkpointer, max_size_mb=10)

    graph = builder.compile(checkpointer=checkpointer)

    # This will succeed (small checkpoint)
    await graph.ainvoke({"messages": [("user", "Hello")]}, config)

    # This will fail with clear error (large checkpoint)
    large_state = {"messages": [("user", "x" * 15_000_000)]}  # 15MB
    try:
        await graph.ainvoke(large_state, config)
    except ValueError as e:
        print(f"Expected error: {e}")
```

---

## Execution Protocol

**Prerequisites:**
- M2.1 complete (AsyncSqliteSaver working)
- M2.2 complete (Blob storage integration available)
- Test cases with varying checkpoint sizes

**Execution Steps:**
1. Implement `SizeLimitedCheckpointer` wrapper
2. Set max_size_mb=10 (10MB limit)
3. Test with 5MB checkpoint (should succeed)
4. Test with 15MB checkpoint (should fail with error)
5. Verify error message is helpful
6. Query database for largest checkpoint: `SELECT MAX(LENGTH(checkpoint)) FROM checkpoints`
7. Verify no checkpoint >10MB exists
8. Measure validation overhead (should be <50ms)

**Verification Steps:**
1. Create test with 15MB checkpoint
2. Attempt to save: `await checkpointer.aput(config, large_checkpoint, metadata, {})`
3. Verify ValueError raised
4. Verify error message contains:
   - Current size: "15MB"
   - Limit: "10MB"
   - Suggestion: "externalize to S3"
5. Query max checkpoint size: `SELECT MAX(LENGTH(checkpoint)) FROM checkpoints`
6. Verify result <10MB
7. Test normal operation with <10MB checkpoint (should work)

---

## The Completion Signal

**Signal:** 15MB checkpoint rejected with helpful error, no checkpoint >10MB in database

**Evidence Required:**
- Test script attempting 15MB checkpoint save
- Error message screenshot showing size violation
- Database query showing max checkpoint size <10MB
- Performance test showing validation overhead <50ms
- Before/after comparison: no limit → 10MB limit enforced

**State Transition:**
```yaml
before:
  status: complete
  witness: impossible (no size validation)
  max_checkpoint_size: unbounded (crashes possible)
  validation: none

after:
  status: complete
  witness: observed (15MB rejected, <10MB enforced)
  max_checkpoint_size: 10MB (enforced)
  validation: pre-serialization (fast fail)
  evidence: [error_message, db_query, validation_benchmark]
```

---

## Constraint Compliance

**CONTEXT_PRESERVATION:** Size limits prevent corruption - rejecting oversized checkpoints preserves database integrity. Context survives because validation prevents crashes.

**CONSTRAINT_INHERITANCE:** Size limits apply to all child agents - inherited enforcement prevents cascading failures. No child can violate parent size constraint.

**TRACE_REQUIRED:** Size violations logged with full context - what checkpoint, how large, when rejected. Audit trail of enforcement actions.

**RESOURCE_STEWARDSHIP:** Size limits prevent resource waste - reject oversized checkpoints before expensive serialization/storage. Minimal necessary validation.

**RESIDUE_FREE:** Clean rejection leaves no partial writes - validation before storage means no corrupted checkpoints. Database remains consistent.

---

## Notes

**Critical Insight:** This task removes the constraint of "unchecked checkpoint growth". Size validation → fast fail → clear error → developer fixes root cause → sustainable operation.

**Why 10MB:** Balance between flexibility and performance. <10MB checkpoints serialize in <100ms. >10MB checkpoints should use S3 externalization (M2.2).

**Common Causes of Large Checkpoints:**
1. Storing file bytes directly (use S3 references)
2. Accumulating unbounded message history (use summarization)
3. Storing large embeddings (use vector DB references)
4. Caching API responses (use external cache)

**Error Message Quality:** The error message should be actionable - tell developer exactly what to do (externalize to S3). Not just "too large".

**Testing Gotcha:** Don't test with synthetic data (random bytes). Test with realistic large states (actual PDFs, images, message histories).

**Performance Trade-off:** Validation requires serialization to measure size - adds ~10ms overhead per checkpoint. Worth it to prevent crashes.

**Monitoring Alert:** Set alert if >5% of checkpoint attempts are rejected for size - indicates widespread blob storage issue.
